---
aliases:
  - Large language models (LLMs) are machine learning which produce human-like language through training on giant datasets
  - large language model (LLM)
  - LLM (large language models)
  - large language models (LLMs)
  - LLM technology
  - large language model (LLM)
  - LLMs (large language models)
  - text generation technology
tags:
  - intelligence/artificial-intelligence
  - linguistics
  - intelligence/artificial-intelligence
  - information
  - computer-science
note-type:
  - general
description: 
file-created: 2023-03-25
file-modified: 2023-11-19
linter-yaml-title-alias: Large language models (LLMs) are machine learning which produce human-like language through training on giant datasets
---

# Large language models (LLMs) are machine learning which produce human-like language through training on giant datasets

#status/postponed

---

> [!ai] AI
> Large language models (LLMs) are [[Machine Learning|machine learning]] models that have been trained on massive amounts of data to generate human-like language. They are designed to understand and produce natural language in a wide range of contexts, including text, speech, and images. LLMs use [[deep learning]] techniques to learn patterns in language data and can perform tasks such as text generation, translation, summarization, question-answering, and sentiment analysis.
>
> LLMs have numerous applications across industries, including chatbots for customer service and virtual assistants like Amazon's Alexa or Apple's Siri. They also have potential in fields such as journalism, where they could be used to automatically generate news articles or summaries. However, there are also concerns about the potential misuse of LLMs for generating fake news or [[Firehose of Falsehood|propaganda]].

A general introductory guide to LLMs [How to run your own LLM (GPT)](https://blog.rfox.eu/en/Programming/How_to_run_your_own_LLM_GPT.html) which can be helpful when trying to [[Artificial intelligence (AI) learning resources|learn artificial intelligence (AI)]].

## State of the [[Large language models produce human text using big data|large language model]] industry

It's the wild west right now but I'm learning how to [[Interacting with large language models|interact with large language models (LLM)]] using tools such as LM studio. It's been a big benefit to my personal workflows - but I wonder if society will become accustomed to all its various functions and the [[Overton window|Overton window]] will shift again, being considering as banal as a spellchecker.

Many [[Conversational AI|Conversational AI]] will use autoregression to predict token generation.

### News/blogs/articles in the LLM industry

- [ML Blog - Articles](https://mlabonne.github.io/blog/)
- [LocalLlama](https://www.reddit.com/r/LocalLLaMA/)
- [What Is ChatGPT Doing … and Why Does It Work?—Stephen Wolfram Writings](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)

## Key concepts in LLMs

- Review: [https://arxiv.org/pdf/2307.14385.pdf](https://arxiv.org/pdf/2307.14385.pdf)

Join these guys focused on making LLMs have emotional intelligence

See also [[Artificial intelligence (AI) learning resources|tech stack for artificial intelligence (AI)]]

They're part of the [[Artificial intelligence (AI) learning resources|tools for AI]] and it might be worth spending time understanding what they are as they seem to be part of the foundational tech stack

### Perplexity in LLMs is a predicition performance metric

> [!ai]+ AI
>
> Perplexity is a metric used to measure the performance of large language models (LLMs). It quantifies how well a language model predicts the next word in a sequence of words. The lower the perplexity score, the better the model is at predicting the next word. Perplexity is calculated based on the probability distribution of words in a given context. LLMs with high perplexity scores may struggle to generate coherent and grammatically correct text.

### Popular LLM architectures/foundational models

Popular models include:
- [[OpenAI|OpenAI]] with GPT-3 and GPT-4 using the GPT architecture
- BERT from Google
- [[RoBERTa|RoBERTa]] which comes from Facebook AI
	- They're pioneering a lot of great work
	- For example, my friend %% [[Stuart Alldritt|Stuart Alldritt]] %% was showing how one of their [[Machine Learning|machine learning]] models was able to segment out objects in a 3D environment - quite useful for augmented reality and virtual reality
- There's also been big competition in the open-source LLM community coming up with innovative models which are able to compete with way less parameters. Everyday on [[Machine learning collaboration platforms|HuggingFace]] we see people coming up with new [[Fine tuning large language models|fine tuned LLM]].

## Related concepts

- See also [[Artificial intelligence (AI) learning resources]] 
- See also [[Interacting with large language models]]
- See also [[Transformer architecture in the context of LLMs]]
- See also [[Optimizing large language models]]
- See also [[Retrieval augmented generation (RAG)]]
