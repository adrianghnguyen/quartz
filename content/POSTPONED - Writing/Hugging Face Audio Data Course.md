---
dg-publish: 
aliases:
  - Hugging Face Audio Data Course
tags:
  - 
note-type:
  - general
description: 
file-created: 2023-10-19
file-modified: 2023-10-19
linter-yaml-title-alias: Hugging Face Audio Data Course
review: 
---

# Hugging Face Audio Data Course

#status/postponed 

---

A free course to learn how to use AI and transformer architectures to work with audio for [[automatic speech recognition (ASR)|automatic speech recognition (ASR)]].

## Unit 1 - Working with audio data

> In this unit, you will gain an understanding of the fundamental terminology related to audio data, including waveform, sampling rate, and spectrogram. You will also learn how to work with audio datasets, including loading and preprocessing audio data, and how to stream large datasets efficiently.
>
> [Introduction to audio data - Hugging Face Audio Course](https://huggingface.co/learn/audio-course/chapter1/audio_data)

Sound is often visualized as a waveform which shows the sample values over time and illustrates changes in its amplitude. Waveforms are an example of time domain representation of sound.

- Sampling rate - how often we get information (samples) from an audio wave
- Amplitude - how loud/sound pressure to humans
- Bit depth - precision of the audio captured in the digital audio file

[[Machine learning collaboration platforms|HuggingFace]] has a library which allows to directly load audio datasets using this command:

```
pip install datasets[audio]
```
This will be useful when developing [[Machine Learning|machine learning]] and doing [[Automatic speech transcriptions in mental healthcare|ASR in therapy]].
### Visualizing sound

#### Waveform

![[Hugging Face Audio Data Course - waveform.png|600]]

#### Frequency spectrum of an audio signal

- This is using the discrete Fourier transform (DFT) which is another word for [[Fourier transform|Fast Fourier Transform (FFT)]]

![[Hugging Face Audio Data Course - DFT.png|600]]

#### Frequency spectrum

> A spectrogram plots the frequency content of an audio signal as it changes over time. It allows you to see time, frequency, and amplitude all on one graph. The algorithm that performs this computation is the STFT or Short Time Fourier Transform.

This is done using a spectrogram which allows us to see how the frequencies in the auto signal change over time. In a way, it's a way of combining both sources of information from the frequency spectrum and the waveform.

![[Hugging Face Audio Data Course - spectrogram.png|520]]

> Since the spectrogram and the waveform are different views of the same data, itâ€™s possible to turn the spectrogram back into the original waveform using the inverse STFT. However, this requires the phase information in addition to the amplitude information. If the spectrogram was generated by a machine learning model, it typically only outputs the amplitudes. In that case, we can use a phase reconstruction algorithm such as the classic Griffin-Lim algorithm, or using a neural network called a vocoder, to reconstruct a waveform from the spectrogram.

In speech processing and [[Machine Learning|machine learning]] tasks, they use a variant called a 'mel spectrogram'. "The mel scale is a **perceptual scale** that approximates the non-linear frequency response of the human ear."

> It is similar to a spectrogram in that it shows the frequency content of an audio signal over time, but on a different frequency axis.
>
> In a standard spectrogram, the frequency axis is linear and is measured in hertz (Hz). However, the human auditory system is more sensitive to changes in lower frequencies than higher frequencies, and this sensitivity decreases logarithmically as frequency increases.
