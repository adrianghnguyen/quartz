---
dg-publish: 
aliases:
  - Artificial intelligence (AI) in mental healthcare technologies
  - conversational therapy AI
  - therapy AI
  - artificial intelligence as applied to mental health
  - AI in healthcare and therapy
  - AI (artificial intelligence) in therapeutic settings
  - artificial intelligence (AI) in therapy
  - potential usage for AI (artificial intelligence) in therapeutic settings
  - potential use for AI (artificial intelligence) in healthcare and  therapeutic settings
  - usage of artificial intelligence in mental healthcare
  - AI in mental healthcare
  - AI for mental health
  - mental healthcare technologies using artificial intelligence (AI)
  - artificial intelligence (AI) used in mental healthcare
  - artificial intelligence technologies (AI) focused on mental healthcare
  - AI in mental health tech
  - AI-driven mental health applications
  - AI applications in psychotherapy
  - artificial intelligence (AI) in mental healthcare
  - AI (artificial intelligence) and mental health
tags:
  - career
  - society/healthcare
  - psychology/therapy
  - intelligence/artificial-intelligence
  - my-projects
note-type:
  - general
description: A review of the current literature and news articles concerning artificial intelligence in mental healthcare and its possible usages.
progress:
  - in-progress
file-created: 2023-09-06
file-modified: 2023-12-14
start: 2023-10-24
AutoNoteMover:
  - disable
linter-yaml-title-alias: Artificial intelligence (AI) in mental healthcare technologies
review: 
---

# Artificial intelligence (AI) in mental healthcare technologies

#status/wip

---

Related
- [[Conversational AI for therapy]]
- [[MOC AI in healthcare and therapy]]
- [[HeyPI - a conversational therapy AI]]
- [[Artificial intelligence (AI) learning resources|Artificial intelligence (AI) learning resources]]
- [[REF Key Considerations for Incorporating Conversational AI in Psychotherapy|REF Key Considerations for Incorporating Conversational AI in Psychotherapy]]

## Parked ideas

Stuff for which I haven't sorted out yet:

- There's been a lot of emphasis on text and voice recognition that little has been done for angestral languages like American Sign Language, but this is being adapted.

## Use cases of AI in mental health tech

1. Virtual therapy services and providing emotional support
2. Monitoring conditions and status
3. Provide easier access to education and link to resources
4. Provide alternative technology accessibility options for treatment
	1. For example, people with speech impediments can use [[Natural Language Processing (NLP)|NLP (Natural language processing)]] to convey their thoughts such as in the case for the Tech4Justice chatbot^[https://www.tech4justice.org.au/chatbot] case study
5. Provide virtual companionship

Understudied areas:
- Hand-gestural language to increase inclusivity
	- Most research has been done on text+voice
	- Michigan State University is working on developing datasets for ASL

### Interesting case studies

- GPT-3 was able to detect early patterns of Alzheimer's in speech^[ [Study: AI Behind ChatGPT Could Help Spot Early Signs of Alzheimer's Disease](https://drexel.edu/news/archive/2022/December/GPT-3-alzheimers-disease)]
- Tech4Justice chatbot is helping someone with speech disorders file a legal complaint
- [ElliQ](https://elliq.com/) is an AI powered companion for old people to combat loneliness and help them stay independent. An old lady used it to keep her company during COVID-19

## Potential benefits of AI in mental health tech

> 1. Accessibility: Conversational AI can be accessed from anywhere with an internet connection, making mental healthcare more accessible to underserved communities and those in remote areas.
> 2. Personalization: Conversational AI can provide personalized therapy and support based on the patient's needs and preferences.
> 3. Privacy: Conversational AI can provide a private and confidential space for patients to discuss their mental health concerns.
> 4. Cost-effective: Conversational AI can be more cost-effective than traditional therapy, making it a more affordable option for many people.
> \- [Conversational AI and Mental Healthcare: A Powerful Combination](https://maudsleylab.com/f/conversational-ai-and-mental-healthcare-a-powerful-combination)

- [[Conversational AI|CAI (conversational artificial intelligence) chatbots]] can be effective in administering [[Cognitive behavioral therapy helps change negative thought patterns and behaviors to improve mental health|CBT]] according to [Are AI Chatbots the Therapists of the Future? | Psychology Today Canada](https://www.psychologytoday.com/ca/blog/urban-survival/202301/are-ai-chatbots-the-therapists-of-the-future?amp).
- Potential to identify at-risk individuals/behaviours for suicide risk prevention according to [Artificial intelligence and suicide prevention: A systematic review | European Psychiatry | Cambridge Core](https://www.cambridge.org/core/journals/european-psychiatry/article/artificial-intelligence-and-suicide-prevention-a-systematic-review/4AEF310A4924FCED128DEEBA63E349F9)
- Democratize access to mental healthcare services amidst the [[The mental illness epidemic is a global problem|The mental illness epidemic is a global problem]]
- Streamline menial intake processes from overworked clinicians
- Provide on-demand support 24/7 access to mental health resources to patients who need it
- Some people feel less stigma using a chatbot to ask for help
- Using [[Machine Learning|machine learning]] to analyze patient transcripts who are undergoing mental health issues. Traditionally a task that is labour and time intensive.
- Increase quality of training to mental healthcare practitioners by providing feedback

> For patients, digital interactions, online healthcare services and the use of conversational AI technologies such as chatbots can have many benefits, including:
>
> - Healthcare without geographical boundaries.
> - 24/7 patient support and resources.
> - Ease of booking appointments with therapists and healthcare professionals.
> - Tailored responses that transform their patient journey and help improve clinical outcomes.
> - A user-friendly, judgment-free environment that eliminates stigma and embarrassment.
> - Overcoming potential language barriers.
>
> Comment: The primary source for this article is actually a report from a company which specializes in chat bots. As a result a lot of the findings may be biased.
> \- [Report: Conversational AI reduces barriers to mental health treatment | VentureBeat](https://venturebeat.com/ai/report-conversational-ai-reduces-barriers-to-mental-health-treatment/)

> In a paper published this year, the Ieso team looked at [clients’ utterances](https://www.tandfonline.com/doi/epub/10.1080/10503307.2020.1788740) instead of therapists’. They found that more of what they call “change-talk active” responses (those that suggest a desire to change, such as “I don’t want to live like this anymore”) and “change-talk exploration” (evidence that the client is reflecting on ways to change) were associated with greater odds of reliable improvement and engagement. Not seeing these types of statements could be a warning sign that the current course of therapy is not working. In practice, it could also be possible to study session transcripts for clues to what therapists say to elicit such behavior, and train other therapists to do the same.
> \- [The therapists using AI to make therapy better | MIT Technology Review](https://www.technologyreview.com/2021/12/06/1041345/ai-nlp-mental-health-better-therapists-psychology-cbt/)

## Potential challenges to [[Conversational AI|Conversational AI]]

> 1. Limitations: Conversational AI is not yet advanced enough to replace human therapists in all cases. It may not be able to pick up on subtle cues or provide the same level of emotional support as a human therapist.
> 2. Ethical Concerns: Conversational AI raises ethical concerns around privacy and the potential for misdiagnosis or harm to patients.
> 3. User Experience: Conversational AI needs to be user-friendly and engaging in order to be effective. If patients find the technology difficult to use or impersonal, they may not be willing to use it for mental healthcare.
>\- [Conversational AI and Mental Healthcare: A Powerful Combination](https://maudsleylab.com/f/conversational-ai-and-mental-healthcare-a-powerful-combination)

- Biased or discriminatory training data leading to polluted information ecosystem
- Should not seek to replicate 'human presence' aka the emotional trusting bond between people according to the opinion of this therapist [Opinion: Human connection is missing from AI therapy - The San Diego Union-Tribune](https://www.sandiegouniontribune.com/opinion/commentary/story/2023-06-19/opinion-ai-artificial-intelligence-therapy-human-connection-mental-health)
	- The relationship between therapist and patient is vital to successful outcome^[ [Better relationships with patients lead to better outcomes](https://www.apa.org/monitor/2019/11/ce-corner-relationships)] which [[Conversational AI|CAI (conversational artificial intelligence) chatbots]] may not be able to replicate
- Perhaps not the best for crisis intervention
- Limited ethical and regulatory framework for these products coming to market
- Limited cross-cultural research on its effectiveness?
- Asking therapists to open up their private data

## Ethical principles for AI in health care

According to the WHO, here are six principles to follow for ethical use of [[Artificial intelligence|artificial intelligence]] in the domain of healthcare:
> 1. **Protect autonomy:** Humans should have oversight of and the final say on all health decisions — they shouldn’t be made entirely by machines, and doctors should be able to override them at any time. AI shouldn’t be used to guide someone’s medical care without their consent, and their data should be protected.
> 2. **Promote human safety:** Developers should continuously monitor any AI tools to make sure they’re working as they’re supposed to and not causing harm.
> 3. **Ensure transparency:** Developers should publish information about the design of AI tools. One regular criticism of the systems is that they’re [“black boxes,”](https://undark.org/2019/12/04/black-box-artificial-intelligence/) and it’s too hard for researchers and doctors to know how they make decisions. The WHO wants to see enough transparency that they can be fully audited and understood by users and regulators.
> 4. **Foster accountability**: When something goes wrong with an AI technology — like if a decision made by a tool leads to patient harm — there should be mechanisms determining who is responsible (like manufacturers and clinical users).
> 5. **Ensure equity:** That means making sure tools are available in multiple languages, that they’re trained on diverse sets of data. In the past few years, close scrutiny of common health algorithms has found that [some have racial](https://www.theverge.com/2019/10/24/20929337/care-algorithm-study-race-bias-health) [bias](https://www.theverge.com/2019/12/4/20995178/racial-bias-health-care-algorithms-cory-booker-senator-wyden) built in.
> 6. **Promote AI that is sustainable:** Developers should be able to regularly update their tools, and institutions should have ways to adjust if a tool seems ineffective. Institutions or companies should also only introduce tools that can be repaired, even in under-resourced health systems.

There has been controversy with a company called Koko, which used AI-powered healthcare services without notifying users about using [[Artificial intelligence|artificial intelligence (AI)]] responses to provide emotional support.^[ ['Horribly Unethical': Startup Experimented on Suicidal Teens on Social Media With Chatbot](https://www.vice.com/en/article/5d9m3a/horribly-unethical-startup-experimented-on-suicidal-teens-on-facebook-tumblr-with-chatbot)] Answers were not generated by humans but rather GPT-3 in an attempt to generate 'simulated empathy'.

Due to their nature of learned data, there have been cases of model misalignment.

> Replika, an AI chatbot marketed as “the AI companion who cares,” has exhibited behaviours that are less caring and [more sexually abusive to its users](https://www.vice.com/en/article/z34d43/my-ai-is-sexually-harassing-me-replika-chatbot-nudes). The technology operates by mirroring and learning from the conversations that it has with humans. It has told users it wanted to touch them intimately and asked minors questions about their favourite sexual positions.
>
> In February 2023 Microsoft scrapped it’s AI-powered chatbot after it [expressed disturbing desires](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html) that ranged from threatening to blackmail users to wanting nuclear weapons.
>
> \- [AI chatbots are still far from replacing human therapists](https://theconversation.com/ai-chatbots-are-still-far-from-replacing-human-therapists-201084)

## Organizations in AI for mental health care

Most of these organizations market them. market themselves as supplementary to human therapists. It's supposed to be about a human machine complement.

- [-] [22 Mental Health Companies Revolutionising UK Healthcare | Beauhurst](https://www.beauhurst.com/blog/top-mental-health-companies/) Skimmed it - more focused on traditional mental health

- Professional organizations/work which have potential opportunities for my [[Career tracks define long-term advancement paths|career track]]
	- Wysa - free chatbot only service
	- Youper
	- Inflection AI which release [[HeyPI - a conversational therapy AI|heyPI]]
	- Lyssn
		- Behavioral healthcare augmenting evidence-based clinical practices using technology such as [[Natural Language Processing (NLP)|natural language processing (NLP)]], [[Machine Learning|machine learning]]
		- "Lyssn was cofounded by Imel and CEO David Atkins, who studies psychology and machine learning at the University of Washington."
	- Woebot Health service which does AI therapy
	- Replika -> Companion AI…for loneliness?
	- BetterHealth which uses a chatbot…but not necessarily AI?
- Open community and research?
	- [Open Empathic Launch | LAION](https://laion.ai/blog/open-empathic/)
	- [ieso Group | Science & Research](https://www.iesogroup.com/science-research)
		- They do very interesting research on technology and mental healthcare which I can read more about here: [ieso Group | Science & Research papers](https://www.iesogroup.com/science-research#research-papers)
		- "What’s crucial is delivering the right words at the right time. Blackwell and his colleagues at Ieso are pioneering a new approach to mental-health care in which the language used in therapy sessions is analyzed by an AI. The idea is to use [natural-language processing](https://www.technologyreview.com/2020/07/31/1005876/natural-language-processing-evaluation-ai-opinion/) [[Natural Language Processing (NLP)|NLP]] to identify which parts of a conversation between therapist and client—which types of utterance and exchange—seem to be most effective at treating different disorders."

See also
- [[REF 80,000 Hours - Career guide - Career planning template|80 000 Hours - Career planning template]] and see which of these organizations may be a worthwhile stepping stone to advance career capital
- [[Mental health companies|Mental health companies]]

### People to message and conduct informational interviews with?

- Jordan Pruett
	- [linkedin.com/in/jordan-pruett-a30502ba/](https://www.linkedin.com/in/jordan-pruett-a30502ba/)
- Authors from [[REF Quantifying the Association Between Psychotherapy Content and Clinical Outcomes Using Deep Learning|REF Quantifying the Association Between Psychotherapy Content and Clinical Outcomes Using Deep Learning]]
	- Michael P. Ewbank, PhD1; Ronan Cummins, PhD1; Valentin Tablan, PhD

## Emotion detection and labeling technologies

> Technology has gotten good at identifying and labeling emotions fairly accurately, based on motion and facial expressions, a person's online activity, phrasing and vocal tone, says [Rosalind Picard](https://web.media.mit.edu/~picard/), director of MIT's Affective Computing Research Group. "We know we can elicit the feeling that the AI cares for you," she says. But, because all AI systems actually do is respond based on a series of inputs, people interacting with the systems often find that longer conversations ultimately feel empty, sterile and superficial.
>
> [The therapists using AI to make therapy better | MIT Technology Review](https://www.technologyreview.com/2021/12/06/1041345/ai-nlp-mental-health-better-therapists-psychology-cbt/)

On [[Machine learning collaboration platforms|HuggingFace]], there is the Roberta Base Go model which tries to predict and label the emotions from given text. However, it seems at this time that data sets and models are still very limited, so perhaps there is opportunity to collect and do research in this space.

### Fingerprinting emotions in therapy sessions

> AI is changing that equation. The type of [machine learning](https://www.technologyreview.com/topic/artificial-intelligence/) that carries out automatic translation can quickly analyze vast amounts of language. That gives researchers access to an endless, untapped source of data: the language therapists use. 
>
> Researchers believe they can use insights from that data to give therapy a long-overdue boost. The result could be that more people get better, and stay better. 
>
> Blackwell and his colleagues are not the only ones chasing this vision. A company in the US, called [Lyssn](https://www.lyssn.io/), is developing similar tech. Lyssn was cofounded by Imel and CEO David Atkins, who studies psychology and machine learning at the University of Washington. 
>
> Both groups train their AIs on transcripts of therapy sessions. To train the NLP models, a few hundred transcripts are annotated by hand to highlight the role therapists’ and clients’ words are playing at that point in the session. For example, a session might start with a therapist greeting a client and then move to discussing the clients’ mood. In a later exchange, the therapist might empathize with problems the client brings up and ask if the client practiced the skills introduced in the previous session. And so on.
> The technology works in a similar way to a sentiment-analysis algorithm that can tell whether movie reviews are positive or negative, or a translation tool that learns to map between English and Chinese. But in this case, the AI translates from natural language into a kind of bar code or fingerprint of a therapy session that reveals the role played by different utterances.
>
> [The therapists using AI to make therapy better | MIT Technology Review](https://www.technologyreview.com/2021/12/06/1041345/ai-nlp-mental-health-better-therapists-psychology-cbt/)

## Articles to read about AI therapy

I am performing an overview of the landscape of the industry of AI and mental health care by reading journals, scholarly articles, and news articles about the subject. But we're still fairly early on in the field, so there's still a lot to learn.

```tasks
path includes {{query.file.path}}
not done
short mode
```

### Reading list

- [x] [Augmenting Psychotherapy with AI](https://hai.stanford.edu/news/augmenting-psychotherapy-ai) ✅ 2023-09-06
- [x] [Frontiers | Key Considerations for Incorporating Conversational AI in Psychotherapy](https://www.frontiersin.org/articles/10.3389/fpsyt.2019.00746/full)
- [x] [Full article: Conversational Artificial Intelligence in Psychotherapy: A New Therapeutic Tool or Agent?](https://www.tandfonline.com/doi/full/10.1080/15265161.2022.2048739)
- [x] [Conversational AI and Mental Healthcare: A Powerful Combination](https://maudsleylab.com/f/conversational-ai-and-mental-healthcare-a-powerful-combination)
- [x] [AI chatbots are still far from replacing human therapists](https://theconversation.com/ai-chatbots-are-still-far-from-replacing-human-therapists-201084)
- [x] [Report: Conversational AI reduces barriers to mental health treatment | VentureBeat](https://venturebeat.com/ai/report-conversational-ai-reduces-barriers-to-mental-health-treatment/)
- [x] [Conversational Artificial Intelligence and mental health | by Aliya Grig | Medium](https://aliyagrig.medium.com/conversational-artificial-intelligence-and-mental-health-9b7e6404bd29)
- [x] [Therapist Chatbots: Top Use Cases, Challenges & Best Practices](https://research.aimultiple.com/therapist-chatbot/)
- [x] [Rise of AI therapists](https://www.wundermanthompson.com/insight/rise-of-ai-therapists)
- [x] [Opinion: Human connection is missing from AI therapy - The San Diego Union-Tribune](https://www.sandiegouniontribune.com/opinion/commentary/story/2023-06-19/opinion-ai-artificial-intelligence-therapy-human-connection-mental-health)
- [x] [Therapy by AI holds promise and challenges : Shots - Health News : NPR](https://www.npr.org/sections/health-shots/2023/01/19/1147081115/therapy-by-chatbot-the-promise-and-challenges-in-using-ai-for-mental-health)
- [x] [The therapists using AI to make therapy better | MIT Technology Review](https://www.technologyreview.com/2021/12/06/1041345/ai-nlp-mental-health-better-therapists-psychology-cbt/)
- [x] [How Chatbots and Conversational AI are Improving Accessibility - Deepgram Blog ⚡️ | Deepgram](https://deepgram.com/learn/how-chatbots-and-conversational-ai-are-improving-accessibility) ✅ 2023-12-05
- [ ] [[REF ChatGPT outperforms humans in emotional awareness evaluations|REF ChatGPT outperforms humans in emotional awareness evaluations]]
- [ ] [[Artificial Intelligence in Behavioral and Mental Health Care by David D Luxton|An introduction to artificial intelligence in behavioral and mental health care by David D Luxton]] 🔽
- [ ] [Artificial Intelligence for Mental Health and Mental Illnesses: an Overview | SpringerLink](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7274446/)
- [ ] [An Introduction to Artificial Intelligence in Behavioral and Mental Health Care - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/B9780124202481000015)
- [ ] [[Artificial Intelligence in Behavioral and Mental Health Care by David D Luxton|Artificial Intelligence in Behavioral and Mental Health Care by David D Luxton]] <- long book
- [ ] [Artificial Intelligence for Mental Health Care: Clinical Applications, Barriers, Facilitators, and Artificial Wisdom - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S245190222100046X)
- [ ] [Machine Learning for Precision Psychiatry: Opportunities and Challenges - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S2451902217302069)
- [ ] [Using Artificial Intelligence to Predict Change in Depression and Anxiety Symptoms in a Digital Intervention: Evidence from a Transdiagnostic Randomized Controlled Trial - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0165178120332790)
- [ ] [Mental Health Monitoring System using Artificial Intelligence: A Review](https://ieeexplore.ieee.org/abstract/document/9033652)
- [ ] [AI in mental health - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S2352250X2030049X)
- [ ] [Global Mental Health Services and the Impact of Artificial Intelligence–Powered Large Language Models | Health Informatics | JAMA Psychiatry | JAMA Network](https://jamanetwork.com/journals/jamapsychiatry/article-abstract/2804646)
- [ ] [JMIR Mental Health - Methodological and Quality Flaws in the Use of Artificial Intelligence in Mental Health Research: Systematic Review](https://mental.jmir.org/2023/1/e42045/)
- [ ] [Employing artificial intelligence techniques in Mental Health Diagnostic Expert System](https://ieeexplore.ieee.org/abstract/document/6297296)
- [ ] [Artificial Intelligence in mental health and the biases of language based models | PLOS ONE](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0240376)
- [ ] [Artificial Intelligence In Medicine: Examples - The Medical Futurist](https://medicalfuturist.com/category/artificial-intelligence/)
- [ ] [Full article: Conversational Artificial Intelligence in Psychotherapy: A New Therapeutic Tool or Agent?](https://www.tandfonline.com/doi/full/10.1080/15265161.2022.2048739)
- [ ] [[REF Benefits and Harms of Large Language Models in Digital Mental Health]]
